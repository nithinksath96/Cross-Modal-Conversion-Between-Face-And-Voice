{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import pyworld\n",
    "import librosa\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from preprocess import *\n",
    "from model import *\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"model_lambda70_f1f4m1m4\"\n",
    "model_dir = \"./model/\" + model_name\n",
    "\n",
    "data_dir =  \"./vcc2018_training\"\n",
    "voice_dir_list = [\"VCC2SF4\", \"VCC2SF1\", \"VCC2SM4\", \"VCC2SM1\"]\n",
    "output_dir = \"./converted_voices/test/\" + model_name + \"_training_progress\"\n",
    "figure_dir = \"./figure/\" + model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_p = 70\n",
    "lambda_s = 70\n",
    "nb_label = len(voice_dir_list)\n",
    "\n",
    "num_epochs = 25\n",
    "batch_size = 5\n",
    "learning_rate =1e-3\n",
    "learning_rate_ = 1e-4\n",
    "learning_rate__ = 1e-5\n",
    "learning_rate___ = 1e-6\n",
    "sampling_rate = 22050\n",
    "num_envelope  = 36\n",
    "num_mcep = 36\n",
    "frame_period = 5.0\n",
    "n_frames = 1024\n",
    "\n",
    "lambda_cls=1\n",
    "lambda_rec=10\n",
    "lambda_gp=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './vcc2018_training/VCC2SF4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-71ed440e1b4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvoice_dir_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"log_f0_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".npz\"\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Preprocess: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpreprocess_voice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './vcc2018_training/VCC2SF4'"
     ]
    }
   ],
   "source": [
    "for v in voice_dir_list:\n",
    "    if \"log_f0_\"+v+\".npz\" in  os.listdir(os.path.join(data_dir, v)):\n",
    "        continue\n",
    "    print(\"Preprocess: \" + v)\n",
    "    preprocess_voice(os.path.join(data_dir, v), v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess: VCC2SF4\n",
      "Preprocessing Data...\n",
      "Data Loading...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './vcc2018_training/VCC2SF4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-79bd2848514b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Preprocess: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpreprocess_voice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/project2/voice_cvae/preprocess.py\u001b[0m in \u001b[0;36mpreprocess_voice\u001b[0;34m(data_dir, name, sampling_rate, num_mcep, frame_period, n_frames)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m     \u001b[0mwavs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_wavs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampling_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project2/voice_cvae/preprocess.py\u001b[0m in \u001b[0;36mload_wavs\u001b[0;34m(wav_dir, sr)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mwavs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wav\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './vcc2018_training/VCC2SF4'"
     ]
    }
   ],
   "source": [
    "data_dir2 =  \"./vcc2018_evaluation\"\n",
    "\n",
    "for v in voice_dir_list:\n",
    "    if \"log_f0_\"+v+\".npz\" in  os.listdir(os.path.join(data_dir2, v)):\n",
    "        continue\n",
    "    print(\"Preprocess: \" + v)\n",
    "    preprocess_voice(os.path.join(data_dir, v), v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FO = np.load(\"voice_cp_FO.npy\", allow_pickle = True)\n",
    "FY = np.load(\"voice_cp_FY.npy\", allow_pickle = True)\n",
    "MO = np.load(\"voice_cp_MO.npy\", allow_pickle = True)\n",
    "MY = np.load(\"voice_cp_MY.npy\", allow_pickle = True)\n",
    "\n",
    "'''\n",
    "FO = torch.tensor(FO).to(DEVICE)\n",
    "FY = torch.tensor(FY).to(DEVICE)\n",
    "MO = torch.tensor(MO).to(DEVICE)\n",
    "MY = torch.tensor(MY).to(DEVICE)\n",
    "'''\n",
    "\n",
    "def data_load_new(batchsize, s ):\n",
    "    x = []\n",
    "    label = []\n",
    "    label_num = s\n",
    "    \n",
    "    if len(s) < batchsize:\n",
    "        batch_iter_size = len(s)\n",
    "    else:\n",
    "        batch_iter_size = batchsize\n",
    "    \n",
    "    for i in range(batch_iter_size):\n",
    "        \n",
    "        if s[i][0]==0 and s[i][1] == 0:\n",
    "            label_num = 0\n",
    "            source = FO\n",
    "            \n",
    "        if s[i][0]==0 and s[i][1] == 1:\n",
    "            label_num = 1\n",
    "            source = FY\n",
    "            \n",
    "        if s[i][0]==1 and s[i][1] == 0:\n",
    "            label_num = 2\n",
    "            source = MO\n",
    "\n",
    "        if s[i][0]==1 and s[i][1] == 1:\n",
    "            label_num = 3\n",
    "            source = MY\n",
    "        \n",
    "        index = random.randint(0,9)\n",
    "        \n",
    "        out = source[index, :, :, :]\n",
    "        x.append(out)\n",
    "        \n",
    "        label.append(label_num)\n",
    "        \n",
    "    return torch.tensor(x), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebA(data.Dataset):\n",
    "    \"\"\"Dataset class for the CelebA dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, image_dir, attr_path, selected_attrs, transform, mode):\n",
    "        \"\"\"Initialize and preprocess the CelebA dataset.\"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.attr_path = attr_path\n",
    "        self.selected_attrs = selected_attrs\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.train_dataset = []\n",
    "        self.test_dataset = []\n",
    "        self.attr2idx = {}\n",
    "        self.idx2attr = {}\n",
    "        self.preprocess()\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.num_images = len(self.train_dataset)\n",
    "        else:\n",
    "            self.num_images = len(self.test_dataset)\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"Preprocess the CelebA attribute file.\"\"\"\n",
    "        lines = [line.rstrip() for line in open(self.attr_path, 'r')]\n",
    "        all_attr_names = lines[1].split()\n",
    "        for i, attr_name in enumerate(all_attr_names):\n",
    "            self.attr2idx[attr_name] = i\n",
    "            self.idx2attr[i] = attr_name\n",
    "\n",
    "        lines = lines[2:]\n",
    "        random.seed(1234)\n",
    "        random.shuffle(lines)\n",
    "        for i, line in enumerate(lines):\n",
    "            split = line.split()\n",
    "            filename = split[0]\n",
    "            values = split[1:]\n",
    "\n",
    "            label = []\n",
    "            for attr_name in self.selected_attrs:\n",
    "                idx = self.attr2idx[attr_name]\n",
    "                label.append(values[idx] == '1')\n",
    "\n",
    "            if (i+1) < 500:\n",
    "                self.test_dataset.append([filename, label])\n",
    "            else:\n",
    "                self.train_dataset.append([filename, label])\n",
    "                \n",
    "            if i > 50000:\n",
    "               break;\n",
    "            \n",
    "            \n",
    "        print('Finished preprocessing the CelebA dataset...')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return one image and its corresponding attribute label.\"\"\"\n",
    "        dataset = self.train_dataset if self.mode == 'train' else self.test_dataset\n",
    "        filename, label = dataset[index]\n",
    "        image = Image.open(os.path.join(self.image_dir, filename))\n",
    "        ##print(type(image))\n",
    "        return self.transform(image), torch.FloatTensor(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of images.\"\"\"\n",
    "        return self.num_images\n",
    "\n",
    "\n",
    "def get_loader(image_dir, attr_path, selected_attrs, crop_size=178, image_size=128, \n",
    "               batch_size=16, dataset='CelebA', mode='train', num_workers=1):\n",
    "    \"\"\"Build and return a data loader.\"\"\"\n",
    "    transform = []\n",
    "    if mode == 'train':\n",
    "        transform.append(T.RandomHorizontalFlip())\n",
    "    transform.append(T.CenterCrop(crop_size))\n",
    "    transform.append(T.Resize(image_size))\n",
    "    transform.append(T.ToTensor())\n",
    "    transform.append(T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)))\n",
    "    transform = T.Compose(transform)\n",
    "\n",
    "    if dataset == 'CelebA':\n",
    "        dataset = CelebA(image_dir, attr_path, selected_attrs, transform, mode)\n",
    "    elif dataset == 'RaFD':\n",
    "        dataset = ImageFolder(image_dir, transform)\n",
    "\n",
    "    data_loader = data.DataLoader(dataset=dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=(mode=='train'),\n",
    "                                  num_workers=num_workers)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing the CelebA dataset...\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms as T\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "image_dir='/home/ubuntu/project2/StarGAN/data/celeba/images'\n",
    "attr_path='/home/ubuntu/project2/StarGAN/data/celeba/list_attr_celeba.txt'\n",
    "selected_attrs=['Male' ,'Young']\n",
    "target_dir_MY = '/home/ubuntu/project2/CELEBA_DATA/MY'\n",
    "target_dir_FY = '/home/ubuntu/project2/CELEBA_DATA/FY'\n",
    "target_dir_MO = '/home/ubuntu/project2/CELEBA_DATA/MO'\n",
    "target_dir_FO = '/home/ubuntu/project2/CELEBA_DATA/FO'\n",
    "\n",
    "\n",
    "\n",
    "mode = 'train'\n",
    "batch_size = 5\n",
    "num_workers=1\n",
    "crop_size=178\n",
    "image_size=64\n",
    "\n",
    "transform = []\n",
    "if mode == 'train':\n",
    "    transform.append(T.RandomHorizontalFlip())\n",
    "transform.append(T.CenterCrop(crop_size))\n",
    "transform.append(T.Resize(image_size))\n",
    "transform.append(T.ToTensor())\n",
    "#transform.append(T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)))\n",
    "transform = T.Compose(transform)\n",
    "\n",
    "\n",
    "dataset = CelebA(image_dir, attr_path, selected_attrs, transform, mode)\n",
    "data_loader = data.DataLoader(dataset=dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=(mode=='train'),\n",
    "                                  num_workers=num_workers)\n",
    "data_iter = iter(data_loader)\n",
    "start_iters = 0\n",
    "num_iters = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9901"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channel_size, output_channel_size,kernel_size=1, stride=1, groups=1, padding=0):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channel_size, \n",
    "                               output_channel_size, kernel_size=kernel_size, stride=stride, \n",
    "                               padding=padding, bias=False, groups=groups)\n",
    "        self.bn1 = nn.BatchNorm2d(output_channel_size)\n",
    "        self.LRelu = nn.LeakyReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out =   self.LRelu(self.bn1(self.conv1(x)))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channel_size, output_channel_size,kernel_size=1, stride=1,padding=0):\n",
    "        super(DeConvBlock, self).__init__()\n",
    "        self.deconv1 = nn.ConvTranspose2d(input_channel_size, \n",
    "                               output_channel_size, kernel_size=kernel_size, stride=stride, \n",
    "                               padding=padding)\n",
    "        self.bn1 = nn.BatchNorm2d(output_channel_size)\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out =   self.softplus(self.bn1(self.deconv1(x)))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceEncoder(nn.Module):\n",
    "    ''' This the encoder part of VAE\n",
    "    '''\n",
    "    def __init__(self, input_dim, n_classes):\n",
    "        '''\n",
    "        Args:\n",
    "            input_dim: A integer indicating the size of input (in case of MNIST 28 * 28).\n",
    "            n_classes: A integer indicating the number of classes. (dimension of one-hot representation of labels)\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "      \n",
    "        self.FaceEncoder_Layers = []\n",
    "        \n",
    "        self.FaceEncoder_Layers.append(nn.Conv2d(in_channels=3, \n",
    "                               out_channels=32, kernel_size=6, stride=2, \n",
    "                               padding=0, bias=False, groups=1))\n",
    "        \n",
    "        self.FaceEncoder_Layers.append(nn.LeakyReLU(inplace=True))\n",
    "        \n",
    "        self.FaceEncoder_Layers.append(ConvBlock(input_channel_size=32, output_channel_size=64,kernel_size=6, stride=2))\n",
    "        self.FaceEncoder_Layers.append(ConvBlock(input_channel_size=64, output_channel_size=128,kernel_size=4, stride=2))\n",
    "        self.FaceEncoder_Layers.append(ConvBlock(input_channel_size=128, output_channel_size=128,kernel_size=4, stride=2, padding=2))\n",
    "        self.FaceEncoder_Layers.append(ConvBlock(input_channel_size=128, output_channel_size=256,kernel_size=2, stride=2, padding=1))\n",
    "        self.FaceEncoder_Layers.append(ConvBlock(input_channel_size=256, output_channel_size=256,kernel_size=2, stride=2, padding=0))\n",
    "        self.FaceEncoder_Layers = nn.Sequential(*self.FaceEncoder_Layers)\n",
    "        \n",
    "        \n",
    "        self.FaceEncoder_Layers2 = []\n",
    "        self.FaceEncoder_Layers2.append(nn.Linear(256, 256))\n",
    "        self.FaceEncoder_Layers2.append(nn.LeakyReLU(inplace=True))\n",
    "        self.FaceEncoder_Layers2.append(nn.Linear(256, 16))\n",
    "        self.FaceEncoder_Layers2.append(nn.LeakyReLU(inplace=True))\n",
    "        self.FaceEncoder_Layers2 = nn.Sequential(*self.FaceEncoder_Layers2)\n",
    "        \n",
    "       \n",
    "        \n",
    "        self.mu_layer = nn.Conv2d(in_channels=16, \n",
    "                     out_channels=8, kernel_size=1, stride=1, \n",
    "                     padding=0, bias=False, groups=1)\n",
    "        self.var_layer = nn.Conv2d(in_channels=16, \n",
    "                      out_channels=8, kernel_size=1, stride=1, \n",
    "                      padding=0, bias=False, groups=1)\n",
    "               \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.FaceEncoder_Layers(x)\n",
    "        out = out.transpose(1,3)       \n",
    "        out = self.FaceEncoder_Layers2(out)\n",
    "        out = out.transpose(1,3)\n",
    "        mean = self.mu_layer(out)\n",
    "        log_var = self.var_layer(out)\n",
    "\n",
    "        return mean, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDecoder(nn.Module):\n",
    "    ''' This the decoder part of VAE\n",
    "    '''\n",
    "    def __init__(self, latent_dim, n_classes):\n",
    "        '''\n",
    "        Args:\n",
    "            latent_dim: A integer indicating the latent size.\n",
    "            hidden_dim: A integer indicating the size of hidden dimension.\n",
    "            output_dim: A integer indicating the size of output (in case of MNIST 28 * 28).\n",
    "            n_classes: A integer indicating the number of classes. (dimension of one-hot representation of labels)\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_classes = n_classes\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.lin = nn.Linear(1, 128)\n",
    "        \n",
    "        self.FaceDecoder_Layers = []\n",
    "        self.FaceDecoder_Layers.append(nn.Linear(self.latent_dim*2, 128))\n",
    "        self.FaceDecoder_Layers.append(nn.Softplus())\n",
    "        self.FaceDecoder_Layers.append(nn.Linear(128, 2048))\n",
    "        self.FaceDecoder_Layers.append(nn.Softplus())\n",
    "        self.FaceDecoder_Layers = nn.Sequential(*self.FaceDecoder_Layers)\n",
    "        \n",
    "        self.FaceDecoder_Layers1 = []\n",
    "        self.FaceDecoder_Layers1.append(nn.Linear(136, 1))\n",
    "        self.FaceDecoder_Layers1.append(nn.Softplus())\n",
    "        self.FaceDecoder_Layers1 = nn.Sequential(*self.FaceDecoder_Layers1)\n",
    "\n",
    "        \n",
    "        self.FaceDecoder_Layers21 = DeConvBlock(input_channel_size=192, output_channel_size=128, kernel_size=3, stride=2,padding=0)\n",
    "        self.FaceDecoder_Layers22 = DeConvBlock(input_channel_size=136, output_channel_size=128, kernel_size=6, stride=2)\n",
    "        self.FaceDecoder_Layers23 = DeConvBlock(input_channel_size=128, output_channel_size=64, kernel_size=6, stride=2)\n",
    "        self.FaceDecoder_Layers24 = DeConvBlock(input_channel_size=72, output_channel_size=32, kernel_size=6, stride=2)\n",
    "        self.FaceDecoder_Layers25 = DeConvBlock(input_channel_size=32, output_channel_size=16, kernel_size=6, stride=2)\n",
    "        self.FaceDecoder_Layers26 = nn.Conv2d(in_channels=24,out_channels=6, kernel_size=6, stride=1)\n",
    "\n",
    "        \n",
    "        \n",
    "        self.mu_conv = nn.Conv2d(in_channels=6,out_channels=3, kernel_size=8, stride=3)\n",
    "        self.var_conv = nn.Conv2d(in_channels=6,out_channels=3, kernel_size=8, stride=3)\n",
    "        \n",
    "        ##bcast linears\n",
    "        self.lin1 = nn.Linear(128,2048)\n",
    "        self.lin2 = nn.Linear(128,81)\n",
    "        self.lin3 = nn.Linear(128,2304)\n",
    "        self.lin4 = nn.Linear(48,204)\n",
    "        self.lin5 = nn.Linear(48,204)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        batch_size = x.size()[0]\n",
    "        \n",
    "        y1 = self.lin(y)\n",
    "        x1 = torch.cat((x,y1), dim=1)    \n",
    "        \n",
    "        z1 = x1.transpose(1,3)      \n",
    "        \n",
    "        out = self.FaceDecoder_Layers(z1)\n",
    "        out = torch.cat((out,self.lin1(x)), dim=1)\n",
    "        out = out.transpose(1,3)\n",
    "        out = self.FaceDecoder_Layers1(out)\n",
    "        out = out.view(batch_size,128,4,4)\n",
    "        x1 = x.view(batch_size,64,4,4)\n",
    "        out = torch.cat((out,x1), dim=1)\n",
    "        \n",
    "        ##gen_img = self.FaceDecoder_Layers2(out)\n",
    "        \n",
    "        out = self.FaceDecoder_Layers21(out)       \n",
    "        x2 = self.lin2(x)\n",
    "        x2 = x2.view(batch_size,8,9,9)    \n",
    "        out = torch.cat((out,x2), dim=1)\n",
    "        \n",
    "        out = self.FaceDecoder_Layers22(out)        \n",
    "        out = self.FaceDecoder_Layers23(out)\n",
    "        x3 = self.lin3(x)\n",
    "        x3 = x3.view(batch_size,8,48,48)    \n",
    "        out = torch.cat((out,x3), dim=1)\n",
    "        \n",
    "        out = self.FaceDecoder_Layers24(out)\n",
    "        out = self.FaceDecoder_Layers25(out)\n",
    "\n",
    "        x4 = self.lin4(x3)\n",
    "        x5 = self.lin5(x4.transpose(2,3))\n",
    "        out = torch.cat((out,x5), dim=1)\n",
    "\n",
    "        gen_img = self.FaceDecoder_Layers26(out)\n",
    "        \n",
    "        \n",
    "        img_mu = self.mu_conv(gen_img)\n",
    "        img_var = self.var_conv(gen_img)\n",
    "        \n",
    "        return img_mu, img_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UtteranceEncoder(nn.Module):\n",
    "    ''' This the decoder part of VAE\n",
    "    '''\n",
    "    def __init__(self, input_dim):\n",
    "        '''\n",
    "        Args:\n",
    "            latent_dim: A integer indicating the latent size.\n",
    "            hidden_dim: A integer indicating the size of hidden dimension.\n",
    "            output_dim: A integer indicating the size of output (in case of MNIST 28 * 28).\n",
    "            n_classes: A integer indicating the number of classes. (dimension of one-hot representation of labels)\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(self.input_dim, 8, (3,9), (1,1), padding=(1, 4))\n",
    "        self.conv1_bn = nn.BatchNorm2d(8)\n",
    "        self.conv1_gated = nn.Conv2d(self.input_dim, 8, (3,9), (1,1), padding=(1, 4))\n",
    "        self.conv1_gated_bn = nn.BatchNorm2d(8)\n",
    "        self.conv1_sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(8, 16, (4,8), (2,2), padding=(1, 3))\n",
    "        self.conv2_bn = nn.BatchNorm2d(16)\n",
    "        self.conv2_gated = nn.Conv2d(8, 16, (4,8), (2,2), padding=(1, 3))\n",
    "        self.conv2_gated_bn = nn.BatchNorm2d(16)\n",
    "        self.conv2_sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(16, 16, (4,8), (2,2), padding=(1, 3))\n",
    "        self.conv3_bn = nn.BatchNorm2d(16)\n",
    "        self.conv3_gated = nn.Conv2d(16, 16, (4,8), (2,2), padding=(1, 3))\n",
    "        self.conv3_gated_bn = nn.BatchNorm2d(16)\n",
    "        self.conv3_sigmoid = nn.Sigmoid() \n",
    "\n",
    "        self.conv4_mu = nn.Conv2d(16, 16//2, (9,5), (9,1), padding=(1, 2))\n",
    "        self.conv4_logvar = nn.Conv2d(16, 16//2, (9,5), (9,1), padding=(1, 2))\n",
    "        \n",
    "    def forward(self, x):\n",
    " \n",
    "        h1_ = self.conv1_bn(self.conv1(x))\n",
    "        h1_gated = self.conv1_gated_bn(self.conv1_gated(x))\n",
    "        h1 = torch.mul(h1_, self.conv1_sigmoid(h1_gated)) \n",
    "       \n",
    "        h2_ = self.conv2_bn(self.conv2(h1))\n",
    "        h2_gated = self.conv2_gated_bn(self.conv2_gated(h1))\n",
    "        h2 = torch.mul(h2_, self.conv2_sigmoid(h2_gated))\n",
    "        \n",
    "        h3_ = self.conv3_bn(self.conv3(h2))\n",
    "        h3_gated = self.conv3_gated_bn(self.conv3_gated(h2))\n",
    "        h3 = torch.mul(h3_, self.conv3_sigmoid(h3_gated)) \n",
    "        \n",
    "        h4_mu = self.conv4_mu(h3)\n",
    "        h4_logvar = self.conv4_logvar(h3)\n",
    "       \n",
    "        return h4_mu, h4_logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UtteranceDecoder(nn.Module):\n",
    "    ''' This the decoder part of VAE\n",
    "    '''\n",
    "    def __init__(self, input_dim, face_latent_dim):\n",
    "        '''\n",
    "        Args:\n",
    "            latent_dim: A integer indicating the latent size.\n",
    "            hidden_dim: A integer indicating the size of hidden dimension.\n",
    "            output_dim: A integer indicating the size of output (in case of MNIST 28 * 28).\n",
    "            n_classes: A integer indicating the number of classes. (dimension of one-hot representation of labels)\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.face_latent_dim = face_latent_dim\n",
    "        \n",
    "        # Decoder\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(self.input_dim + self.face_latent_dim, 16, (9,5), (9,1), padding=(0, 2))\n",
    "        self.upconv1_bn = nn.BatchNorm2d(16)\n",
    "        self.upconv1_gated = nn.ConvTranspose2d(self.input_dim + self.face_latent_dim, 16, (9,5), (9,1), padding=(0, 2))\n",
    "        self.upconv1_gated_bn = nn.BatchNorm2d(16)\n",
    "        self.upconv1_sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(16+self.face_latent_dim, 16, (4,8), (2,2), padding=(1, 3))\n",
    "        self.upconv2_bn = nn.BatchNorm2d(16)\n",
    "        self.upconv2_gated = nn.ConvTranspose2d(16+self.face_latent_dim, 16, (4,8), (2,2), padding=(1, 3))\n",
    "        self.upconv2_gated_bn = nn.BatchNorm2d(16)\n",
    "        self.upconv2_sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(16+self.face_latent_dim, 8, (4,8), (2,2), padding=(1, 3))\n",
    "        self.upconv3_bn = nn.BatchNorm2d(8)\n",
    "        self.upconv3_gated = nn.ConvTranspose2d(16+self.face_latent_dim, 8, (4,8), (2,2), padding=(1, 3))\n",
    "        self.upconv3_gated_bn = nn.BatchNorm2d(8)\n",
    "        self.upconv3_sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.upconv4_mu = nn.ConvTranspose2d(8+self.face_latent_dim, 2//2, (3,9), (1,1), padding=(1, 4))\n",
    "        self.upconv4_logvar = nn.ConvTranspose2d(8+self.face_latent_dim, 2//2, (3,9), (1,1), padding=(1, 4))\n",
    "        \n",
    "        ##linear layers for face latent broadcasting\n",
    "        self.N = 1024\n",
    "        self.bcast_linear1 = nn.Linear(1,int(self.N/4))  ##n/4 = 256 where n = 1024 frames\n",
    "        self.bcast_linear2 = nn.Linear(1,9)\n",
    "        self.bcast_linear3 = nn.Linear(9,18)\n",
    "        self.bcast_linear4 = nn.Linear(int(self.N/4),int(self.N/2))\n",
    "        self.bcast_linear5 = nn.Linear(18,36)\n",
    "        self.bcast_linear6 = nn.Linear(int(self.N/2),self.N)\n",
    "        \n",
    "        \n",
    "    def forward(self, z, face_latent):\n",
    "        \n",
    "        \n",
    "        face_latent_b1 = self.bcast_linear1(face_latent)\n",
    "        \n",
    "        h5_ = self.upconv1_bn(self.upconv1(torch.cat((z, face_latent_b1), dim=1)))\n",
    "        h5_gated = self.upconv1_gated_bn(self.upconv1(torch.cat((z, face_latent_b1), dim=1)))\n",
    "        h5 = torch.mul(h5_, self.upconv1_sigmoid(h5_gated)) \n",
    "         \n",
    "        face_latent_b1 = face_latent_b1.transpose(2,3)\n",
    "        face_latent_b2 = self.bcast_linear2(face_latent_b1)\n",
    "        face_latent_b2 = face_latent_b2.transpose(2,3)\n",
    "        \n",
    "        h6_ = self.upconv2_bn(self.upconv2(torch.cat((h5, face_latent_b2), dim=1)))\n",
    "        h6_gated = self.upconv2_gated_bn(self.upconv2(torch.cat((h5, face_latent_b2), dim=1)))\n",
    "        h6 = torch.mul(h6_, self.upconv2_sigmoid(h6_gated)) \n",
    "        \n",
    "        face_latent_b2 = face_latent_b2.transpose(2,3)\n",
    "        face_latent_b3 = self.bcast_linear3(face_latent_b2)\n",
    "        face_latent_b3 = face_latent_b3.transpose(2,3)\n",
    "        face_latent_b3 = self.bcast_linear4(face_latent_b3)                  \n",
    "                              \n",
    "        h7_ = self.upconv3_bn(self.upconv3(torch.cat((h6, face_latent_b3), dim=1)))\n",
    "        h7_gated = self.upconv3_gated_bn(self.upconv3(torch.cat((h6, face_latent_b3), dim=1)))\n",
    "        h7 = torch.mul(h7_, self.upconv3_sigmoid(h7_gated)) \n",
    "\n",
    "        \n",
    "        face_latent_b3 = face_latent_b3.transpose(2,3)\n",
    "        face_latent_b3 = self.bcast_linear5(face_latent_b3)\n",
    "        face_latent_b3 = face_latent_b3.transpose(2,3)\n",
    "        face_latent_b4 = self.bcast_linear6(face_latent_b3)\n",
    "                              \n",
    "        \n",
    "        h8_mu = self.upconv4_mu(torch.cat((h7, face_latent_b4), dim=1))\n",
    "        h8_logvar = self.upconv4_logvar(torch.cat((h7, face_latent_b4), dim=1))\n",
    "        \n",
    "        return h8_mu, h8_logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceEncoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, (3,9), (1,1), padding=(1, 4))\n",
    "        self.conv1_bn = nn.BatchNorm2d(32)\n",
    "        self.conv1_gated = nn.Conv2d(1, 32, (3,9), (1,1), padding=(1, 4))\n",
    "        self.conv1_gated_bn = nn.BatchNorm2d(32)\n",
    "        self.conv1_sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, (4,8), (2,2), padding=(1, 3))\n",
    "        self.conv2_bn = nn.BatchNorm2d(64)\n",
    "        self.conv2_gated = nn.Conv2d(32, 64, (4,8), (2,2), padding=(1, 3))\n",
    "        self.conv2_gated_bn = nn.BatchNorm2d(64)\n",
    "        self.conv2_sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, (4,8), (2,2), padding=(1, 3))\n",
    "        self.conv3_bn = nn.BatchNorm2d(128)\n",
    "        self.conv3_gated = nn.Conv2d(64, 128, (4,8), (2,2), padding=(1, 3))\n",
    "        self.conv3_gated_bn = nn.BatchNorm2d(128)\n",
    "        self.conv3_sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 128, (4,8), (2,2), padding=(1, 3))\n",
    "        self.conv4_bn = nn.BatchNorm2d(128)\n",
    "        self.conv4_gated = nn.Conv2d(128, 128, (4,8), (2,2), padding=(1, 3))\n",
    "        self.conv4_gated_bn = nn.BatchNorm2d(128)\n",
    "        self.conv4_sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(128, 128, (4,5), (4,1), padding=(1, 5))\n",
    "        self.conv5_bn = nn.BatchNorm2d(128)\n",
    "        self.conv5_gated = nn.Conv2d(128, 128, (4,5), (4,1), padding=(1, 5))\n",
    "        self.conv5_gated_bn = nn.BatchNorm2d(128)\n",
    "        self.conv5_sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(128, 64, (1,5), (1,1), padding=(1, 5))\n",
    "        self.conv6_bn = nn.BatchNorm2d(64)\n",
    "        self.conv6_gated = nn.Conv2d(128, 64, (1,5), (1,1), padding=(1, 5))\n",
    "        self.conv6_gated_bn = nn.BatchNorm2d(64)\n",
    "        self.conv6_sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.conv7 = nn.Conv2d(64, 16, (1,5), (1,1), padding=(1, 5))\n",
    "        self.conv7_bn = nn.BatchNorm2d(16)\n",
    "        self.conv7_gated = nn.Conv2d(64, 16, (1,5), (1,1), padding=(1, 5))\n",
    "        self.conv7_gated_bn = nn.BatchNorm2d(16)\n",
    "        self.conv7_sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.conv8 = nn.Conv2d(16, 8, (1,5), (1,1), padding=(1, 5))\n",
    "\n",
    "        \n",
    "        self.lin1_mu = nn.Linear(7,1)\n",
    "        self.lin2_mu = nn.Linear(152,128)\n",
    "        \n",
    "        self.lin1_var = nn.Linear(7,1)\n",
    "        self.lin2_var = nn.Linear(152,128)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        h1_ = self.conv1_bn(self.conv1(x))\n",
    "        h1_gated = self.conv1_gated_bn(self.conv1_gated(x))\n",
    "        h1 = torch.mul(h1_, self.conv1_sigmoid(h1_gated)) \n",
    "        \n",
    "        h2_ = self.conv2_bn(self.conv2(h1))\n",
    "        h2_gated = self.conv2_gated_bn(self.conv2_gated(h1))\n",
    "        h2 = torch.mul(h2_, self.conv2_sigmoid(h2_gated))\n",
    "        \n",
    "        h3_ = self.conv3_bn(self.conv3(h2))\n",
    "        h3_gated = self.conv3_gated_bn(self.conv3_gated(h2))\n",
    "        h3 = torch.mul(h3_, self.conv3_sigmoid(h3_gated))\n",
    "\n",
    "        h4_ = self.conv4_bn(self.conv4(h3))\n",
    "        h4_gated = self.conv4_gated_bn(self.conv4_gated(h3))\n",
    "        h4 = torch.mul(h4_, self.conv4_sigmoid(h4_gated))\n",
    "        \n",
    "        h5_ = self.conv5_bn(self.conv5(h4))\n",
    "        h5_gated = self.conv5_gated_bn(self.conv5_gated(h4))\n",
    "        h5 = torch.mul(h5_, self.conv5_sigmoid(h5_gated))\n",
    "        \n",
    "        h6_ = self.conv6_bn(self.conv6(h5))\n",
    "        h6_gated = self.conv6_gated_bn(self.conv6_gated(h5))\n",
    "        h6 = torch.mul(h6_, self.conv6_sigmoid(h6_gated))\n",
    "        \n",
    "        h7_ = self.conv7_bn(self.conv7(h6))\n",
    "        h7_gated = self.conv7_gated_bn(self.conv7_gated(h6))\n",
    "        h7 = torch.mul(h7_, self.conv7_sigmoid(h7_gated))\n",
    "        \n",
    "        h8 = self.conv8(h7) \n",
    "        \n",
    "        h8 = h8.transpose(2,3)\n",
    "        \n",
    "        \n",
    "        h9_mu = self.lin1_mu(h8)\n",
    "        h9_mu = h9_mu.transpose(2,3)\n",
    "        h10_mu = self.lin2_mu(h9_mu)\n",
    "        \n",
    "        \n",
    "        h9_var = self.lin1_var(h8)\n",
    "        h9_var = h9_var.transpose(2,3)\n",
    "        h10_var = self.lin2_var(h9_var)        \n",
    "        \n",
    "        \n",
    "        return h10_mu, h10_var\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Face_Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator network with PatchGAN.\"\"\"\n",
    "    def __init__(self, image_size=64, conv_dim=64, c_dim=4, repeat_num=6):\n",
    "        super(Face_Discriminator, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(3, conv_dim, kernel_size=4, stride=2, padding=1))\n",
    "        layers.append(nn.LeakyReLU(0.01))\n",
    "\n",
    "        curr_dim = conv_dim\n",
    "        for i in range(1, repeat_num):\n",
    "            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1))\n",
    "            layers.append(nn.LeakyReLU(0.01))\n",
    "            curr_dim = curr_dim * 2\n",
    "\n",
    "        kernel_size = int(image_size / np.power(2, repeat_num))\n",
    "        self.main = nn.Sequential(*layers)\n",
    "        self.conv1 = nn.Conv2d(curr_dim, 1, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(curr_dim, c_dim, kernel_size=kernel_size, bias=False)\n",
    "        #self.sig = nn.Sigmoid()\n",
    "        #self.sig_2 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.main(x)\n",
    "        out_src = self.conv1(h)\n",
    "        out_cls = self.conv2(h)\n",
    "        out_cls = out_cls.view(out_cls.size(0), out_cls.size(1))\n",
    "        return F.softmax(out_cls, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Voice_Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator network with PatchGAN.\"\"\"\n",
    "    def __init__(self, label_num):\n",
    "        super(Voice_Discriminator, self).__init__()\n",
    "        \n",
    "        self.label_num = label_num\n",
    "        \n",
    "        self.ac_conv1 = nn.Conv2d(1, 8, (4,4), (2,2), padding=(1, 1))\n",
    "        self.ac_conv1_bn = nn.BatchNorm2d(8)\n",
    "        self.ac_conv1_gated = nn.Conv2d(1, 8, (4,4), (2,2), padding=(1, 1))\n",
    "        self.ac_conv1_gated_bn = nn.BatchNorm2d(8)\n",
    "        self.ac_conv1_sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.ac_conv2 = nn.Conv2d(8, 16, (4,4), (2,2), padding=(1, 1))\n",
    "        self.ac_conv2_bn = nn.BatchNorm2d(16)\n",
    "        self.ac_conv2_gated = nn.Conv2d(8, 16, (4,4), (2,2), padding=(1, 1))\n",
    "        self.ac_conv2_gated_bn = nn.BatchNorm2d(16)\n",
    "        self.ac_conv2_sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.ac_conv3 = nn.Conv2d(16, 32, (4,4), (2,2), padding=(1, 1))\n",
    "        self.ac_conv3_bn = nn.BatchNorm2d(32)\n",
    "        self.ac_conv3_gated = nn.Conv2d(16, 32, (4,4), (2,2), padding=(1, 1))\n",
    "        self.ac_conv3_gated_bn = nn.BatchNorm2d(32)\n",
    "        self.ac_conv3_sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.ac_conv4 = nn.Conv2d(32, 16, (4,4), (2,2), padding=(1, 1))\n",
    "        self.ac_conv4_bn = nn.BatchNorm2d(16)\n",
    "        self.ac_conv4_gated = nn.Conv2d(32, 16, (4,4), (2,2), padding=(1, 1))\n",
    "        self.ac_conv4_gated_bn = nn.BatchNorm2d(16)\n",
    "        self.ac_conv4_sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.ac_conv5 = nn.Conv2d(16, self.label_num, (1,4), (1,2), padding=(0, 1))\n",
    "        self.ac_lin1 = nn.Linear(32,1)\n",
    "        self.ac_conv6 = nn.Conv2d(16, 1, (1,4), (1,2), padding=(0, 1))\n",
    "        self.ac_lin2 = nn.Linear(32,1)\n",
    "        #self.sig1 = nn.Sigmoid()\n",
    "        #self.sig2 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x[:,:,:16]\n",
    "   \n",
    "        h9_ = self.ac_conv1_bn(self.ac_conv1(x))\n",
    "        h9_gated = self.ac_conv1_gated_bn(self.ac_conv1_gated(x))\n",
    "        h9 = torch.mul(h9_, self.ac_conv1_sigmoid(h9_gated))\n",
    "        \n",
    "        h10_ = self.ac_conv2_bn(self.ac_conv2(h9))\n",
    "        h10_gated = self.ac_conv2_gated_bn(self.ac_conv2_gated(h9))\n",
    "        h10 = torch.mul(h10_, self.ac_conv2_sigmoid(h10_gated))\n",
    "        \n",
    "        h11_ = self.ac_conv3_bn(self.ac_conv3(h10))\n",
    "        h11_gated = self.ac_conv3_gated_bn(self.ac_conv3_gated(h10))\n",
    "        h11 = torch.mul(h11_, self.ac_conv3_sigmoid(h11_gated))\n",
    "        \n",
    "        h12_ = self.ac_conv4_bn(self.ac_conv4(h11))\n",
    "        h12_gated = self.ac_conv4_gated_bn(self.ac_conv4_gated(h11))\n",
    "        h12 = torch.mul(h12_, self.ac_conv4_sigmoid(h12_gated))\n",
    "        \n",
    "        h13_ = F.softmax(self.ac_conv5(h12), dim=1)\n",
    "        h13 = torch.prod(h13_, dim=-1, keepdim=True)\n",
    "       \n",
    "        return h13.view(-1, self.label_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\na=torch.distributions.normal.Normal(torch.tensor([0.0]), torch.tensor([1.0]))\\nb=a.sample((1,8,1,1))\\nxx=b.squeeze(4).to(DEVICE)\\nxx = xx.to('cpu').numpy()\\nx1 = xx\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "a=torch.distributions.normal.Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "b=a.sample((1,8,1,1))\n",
    "xx=b.squeeze(4).to(DEVICE)\n",
    "xx = xx.to('cpu').numpy()\n",
    "x1 = xx\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.load(\"xx_final.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = torch.tensor(xx).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8, 1, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossModal(nn.Module):\n",
    "    def __init__(self, lambda1, lambda2, xx1):\n",
    "        super().__init__()\n",
    "        self.xx = xx1\n",
    "        self.lambda_1 = lambda1\n",
    "        self.lambda_2 = lambda2\n",
    "        self.Utterance_Encoder = UtteranceEncoder(1)\n",
    "        self.Utterance_Decoder = UtteranceDecoder(8,8)\n",
    "        self.Face_Encoder = FaceEncoder(3,2)\n",
    "        self.Face_Decoder = FaceDecoder(8,2)\n",
    "        self.Voice_Encoder = VoiceEncoder(1)\n",
    "        self.Face_Classifier = Face_Discriminator(c_dim=4)\n",
    "        self.Voice_Classifier = Voice_Discriminator(4)\n",
    "        self.L2_loss1 = nn.MSELoss()\n",
    "        self.L2_loss2 = nn.MSELoss()\n",
    "        \n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "    \n",
    "    \n",
    "    def forward(self, utterance, face ):\n",
    "        batch_size = utterance.size()[0]\n",
    "        UE_mu, UE_var = self.Utterance_Encoder(utterance)\n",
    "        FE_mu, FE_var = self.Face_Encoder(face)\n",
    "        reparam_UE = self.reparameterize(UE_mu, UE_var)\n",
    "        reparam_FE = self.reparameterize(FE_mu, FE_var)\n",
    "        UD_mu, UD_var = self.Utterance_Decoder(reparam_UE,reparam_FE)\n",
    "        recon_voice = self.reparameterize(UD_mu, UD_var)\n",
    "        voice_label_p = self.Voice_Classifier(recon_voice)\n",
    "        \n",
    "        VE_mu, VE_var = self.Voice_Encoder(recon_voice)\n",
    "        reparam_VE = self.reparameterize(VE_mu, VE_var)\n",
    "        recon_image_mu, recon_image_var = self.Face_Decoder(reparam_VE,self.xx[:batch_size, :,:,:])       \n",
    "        recon_image = self.reparameterize(recon_image_mu, recon_image_var)\n",
    "        image_label_p = self.Face_Classifier(recon_image)\n",
    "        \n",
    "        return recon_voice, recon_image, UE_mu, UE_var, FE_mu, FE_var, voice_label_p, image_label_p\n",
    "    \n",
    "    \n",
    "    def calculate_loss(self, utterance, face, label):\n",
    "        \n",
    "        self.real_label = label\n",
    "        \n",
    "        re_voice, re_face, u_mu, u_var, f_mu, f_var, re_voice_label_p, re_face_label_p = self.forward(utterance, face)\n",
    "        \n",
    "        t_label_voice = self.Voice_Classifier(utterance)\n",
    "        t_label_face = self.Face_Classifier(face)\n",
    "        \n",
    "        L1_voice = torch.sum(torch.abs(re_voice - utterance))\n",
    "        L1_face = torch.sum(torch.abs(re_face - face))\n",
    "        \n",
    "        #L2_voice = self.L2_loss1(re_voice, utterance)\n",
    "        #L2_face = self.L2_loss2(re_face, face)\n",
    "        \n",
    "        #rec_loss_voice = F.binary_cross_entropy_with_logits(re_voice, utterance)\n",
    "        #rec_loss_face = F.binary_cross_entropy_with_logits(re_face, face)\n",
    "        \n",
    "        KLD_voice = -0.5 * torch.sum(1 + u_var - u_mu.pow(2) - u_var.exp())\n",
    "        KLD_face = -0.5 * torch.sum(1 + f_var - f_mu.pow(2) - f_var.exp())\n",
    "        \n",
    "        AC_1_voice = self.lambda_1 * F.binary_cross_entropy(re_voice_label_p, self.real_label) \n",
    "        AC_2_voice = self.lambda_2 * F.binary_cross_entropy(t_label_voice, self.real_label)\n",
    "\n",
    "        #print(re_face_label_p)\n",
    "        AC_1_face = self.lambda_1 * F.binary_cross_entropy(re_face_label_p, self.real_label) \n",
    "        AC_2_face = self.lambda_2 * F.binary_cross_entropy(t_label_face, self.real_label)\n",
    "        \n",
    "        return L1_voice + L1_face + KLD_voice + KLD_face + AC_1_face + AC_2_face + AC_1_voice + AC_2_voice\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM_gen = CrossModal(lambda_p, lambda_s, xx)\n",
    "CM_gen = CM_gen.to(DEVICE)\n",
    "CM_gen_optimizer = torch.optim.Adam(CM_gen.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossModal(\n",
       "  (Utterance_Encoder): UtteranceEncoder(\n",
       "    (conv1): Conv2d(1, 8, kernel_size=(3, 9), stride=(1, 1), padding=(1, 4))\n",
       "    (conv1_bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv1_gated): Conv2d(1, 8, kernel_size=(3, 9), stride=(1, 1), padding=(1, 4))\n",
       "    (conv1_gated_bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv1_sigmoid): Sigmoid()\n",
       "    (conv2): Conv2d(8, 16, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3))\n",
       "    (conv2_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2_gated): Conv2d(8, 16, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3))\n",
       "    (conv2_gated_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2_sigmoid): Sigmoid()\n",
       "    (conv3): Conv2d(16, 16, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3))\n",
       "    (conv3_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3_gated): Conv2d(16, 16, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3))\n",
       "    (conv3_gated_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3_sigmoid): Sigmoid()\n",
       "    (conv4_mu): Conv2d(16, 8, kernel_size=(9, 5), stride=(9, 1), padding=(1, 2))\n",
       "    (conv4_logvar): Conv2d(16, 8, kernel_size=(9, 5), stride=(9, 1), padding=(1, 2))\n",
       "  )\n",
       "  (Utterance_Decoder): UtteranceDecoder(\n",
       "    (upconv1): ConvTranspose2d(16, 16, kernel_size=(9, 5), stride=(9, 1), padding=(0, 2))\n",
       "    (upconv1_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (upconv1_gated): ConvTranspose2d(16, 16, kernel_size=(9, 5), stride=(9, 1), padding=(0, 2))\n",
       "    (upconv1_gated_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (upconv1_sigmoid): Sigmoid()\n",
       "    (upconv2): ConvTranspose2d(24, 16, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3))\n",
       "    (upconv2_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (upconv2_gated): ConvTranspose2d(24, 16, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3))\n",
       "    (upconv2_gated_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (upconv2_sigmoid): Sigmoid()\n",
       "    (upconv3): ConvTranspose2d(24, 8, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3))\n",
       "    (upconv3_bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (upconv3_gated): ConvTranspose2d(24, 8, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3))\n",
       "    (upconv3_gated_bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (upconv3_sigmoid): Sigmoid()\n",
       "    (upconv4_mu): ConvTranspose2d(16, 1, kernel_size=(3, 9), stride=(1, 1), padding=(1, 4))\n",
       "    (upconv4_logvar): ConvTranspose2d(16, 1, kernel_size=(3, 9), stride=(1, 1), padding=(1, 4))\n",
       "    (bcast_linear1): Linear(in_features=1, out_features=256, bias=True)\n",
       "    (bcast_linear2): Linear(in_features=1, out_features=9, bias=True)\n",
       "    (bcast_linear3): Linear(in_features=9, out_features=18, bias=True)\n",
       "    (bcast_linear4): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (bcast_linear5): Linear(in_features=18, out_features=36, bias=True)\n",
       "    (bcast_linear6): Linear(in_features=512, out_features=1024, bias=True)\n",
       "  )\n",
       "  (Face_Encoder): FaceEncoder(\n",
       "    (FaceEncoder_Layers): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(6, 6), stride=(2, 2), bias=False)\n",
       "      (1): LeakyReLU(negative_slope=0.01, inplace)\n",
       "      (2): ConvBlock(\n",
       "        (conv1): Conv2d(32, 64, kernel_size=(6, 6), stride=(2, 2), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LRelu): LeakyReLU(negative_slope=0.01, inplace)\n",
       "      )\n",
       "      (3): ConvBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LRelu): LeakyReLU(negative_slope=0.01, inplace)\n",
       "      )\n",
       "      (4): ConvBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LRelu): LeakyReLU(negative_slope=0.01, inplace)\n",
       "      )\n",
       "      (5): ConvBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LRelu): LeakyReLU(negative_slope=0.01, inplace)\n",
       "      )\n",
       "      (6): ConvBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LRelu): LeakyReLU(negative_slope=0.01, inplace)\n",
       "      )\n",
       "    )\n",
       "    (FaceEncoder_Layers2): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01, inplace)\n",
       "      (2): Linear(in_features=256, out_features=16, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01, inplace)\n",
       "    )\n",
       "    (mu_layer): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (var_layer): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (Face_Decoder): FaceDecoder(\n",
       "    (lin): Linear(in_features=1, out_features=128, bias=True)\n",
       "    (FaceDecoder_Layers): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "      (1): Softplus(beta=1, threshold=20)\n",
       "      (2): Linear(in_features=128, out_features=2048, bias=True)\n",
       "      (3): Softplus(beta=1, threshold=20)\n",
       "    )\n",
       "    (FaceDecoder_Layers1): Sequential(\n",
       "      (0): Linear(in_features=136, out_features=1, bias=True)\n",
       "      (1): Softplus(beta=1, threshold=20)\n",
       "    )\n",
       "    (FaceDecoder_Layers21): DeConvBlock(\n",
       "      (deconv1): ConvTranspose2d(192, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (softplus): Softplus(beta=1, threshold=20)\n",
       "    )\n",
       "    (FaceDecoder_Layers22): DeConvBlock(\n",
       "      (deconv1): ConvTranspose2d(136, 128, kernel_size=(6, 6), stride=(2, 2))\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (softplus): Softplus(beta=1, threshold=20)\n",
       "    )\n",
       "    (FaceDecoder_Layers23): DeConvBlock(\n",
       "      (deconv1): ConvTranspose2d(128, 64, kernel_size=(6, 6), stride=(2, 2))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (softplus): Softplus(beta=1, threshold=20)\n",
       "    )\n",
       "    (FaceDecoder_Layers24): DeConvBlock(\n",
       "      (deconv1): ConvTranspose2d(72, 32, kernel_size=(6, 6), stride=(2, 2))\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (softplus): Softplus(beta=1, threshold=20)\n",
       "    )\n",
       "    (FaceDecoder_Layers25): DeConvBlock(\n",
       "      (deconv1): ConvTranspose2d(32, 16, kernel_size=(6, 6), stride=(2, 2))\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (softplus): Softplus(beta=1, threshold=20)\n",
       "    )\n",
       "    (FaceDecoder_Layers26): Conv2d(24, 6, kernel_size=(6, 6), stride=(1, 1))\n",
       "    (mu_conv): Conv2d(6, 3, kernel_size=(8, 8), stride=(3, 3))\n",
       "    (var_conv): Conv2d(6, 3, kernel_size=(8, 8), stride=(3, 3))\n",
       "    (lin1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "    (lin2): Linear(in_features=128, out_features=81, bias=True)\n",
       "    (lin3): Linear(in_features=128, out_features=2304, bias=True)\n",
       "    (lin4): Linear(in_features=48, out_features=204, bias=True)\n",
       "    (lin5): Linear(in_features=48, out_features=204, bias=True)\n",
       "  )\n",
       "  (Voice_Encoder): VoiceEncoder(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(3, 9), stride=(1, 1), padding=(1, 4))\n",
       "    (conv1_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv1_gated): Conv2d(1, 32, kernel_size=(3, 9), stride=(1, 1), padding=(1, 4))\n",
       "    (conv1_gated_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv1_sigmoid): Sigmoid()\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3))\n",
       "    (conv2_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2_gated): Conv2d(32, 64, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3))\n",
       "    (conv2_gated_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2_sigmoid): Sigmoid()\n",
       "    (conv3): Conv2d(64, 128, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3))\n",
       "    (conv3_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3_gated): Conv2d(64, 128, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3))\n",
       "    (conv3_gated_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3_sigmoid): Sigmoid()\n",
       "    (conv4): Conv2d(128, 128, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3))\n",
       "    (conv4_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv4_gated): Conv2d(128, 128, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3))\n",
       "    (conv4_gated_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv4_sigmoid): Sigmoid()\n",
       "    (conv5): Conv2d(128, 128, kernel_size=(4, 5), stride=(4, 1), padding=(1, 5))\n",
       "    (conv5_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv5_gated): Conv2d(128, 128, kernel_size=(4, 5), stride=(4, 1), padding=(1, 5))\n",
       "    (conv5_gated_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv5_sigmoid): Sigmoid()\n",
       "    (conv6): Conv2d(128, 64, kernel_size=(1, 5), stride=(1, 1), padding=(1, 5))\n",
       "    (conv6_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv6_gated): Conv2d(128, 64, kernel_size=(1, 5), stride=(1, 1), padding=(1, 5))\n",
       "    (conv6_gated_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv6_sigmoid): Sigmoid()\n",
       "    (conv7): Conv2d(64, 16, kernel_size=(1, 5), stride=(1, 1), padding=(1, 5))\n",
       "    (conv7_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv7_gated): Conv2d(64, 16, kernel_size=(1, 5), stride=(1, 1), padding=(1, 5))\n",
       "    (conv7_gated_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv7_sigmoid): Sigmoid()\n",
       "    (conv8): Conv2d(16, 8, kernel_size=(1, 5), stride=(1, 1), padding=(1, 5))\n",
       "    (lin1_mu): Linear(in_features=7, out_features=1, bias=True)\n",
       "    (lin2_mu): Linear(in_features=152, out_features=128, bias=True)\n",
       "    (lin1_var): Linear(in_features=7, out_features=1, bias=True)\n",
       "    (lin2_var): Linear(in_features=152, out_features=128, bias=True)\n",
       "  )\n",
       "  (Face_Classifier): Face_Discriminator(\n",
       "    (main): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (5): LeakyReLU(negative_slope=0.01)\n",
       "      (6): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (7): LeakyReLU(negative_slope=0.01)\n",
       "      (8): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (9): LeakyReLU(negative_slope=0.01)\n",
       "      (10): Conv2d(1024, 2048, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (11): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (conv1): Conv2d(2048, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (conv2): Conv2d(2048, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (Voice_Classifier): Voice_Discriminator(\n",
       "    (ac_conv1): Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (ac_conv1_bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (ac_conv1_gated): Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (ac_conv1_gated_bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (ac_conv1_sigmoid): Sigmoid()\n",
       "    (ac_conv2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (ac_conv2_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (ac_conv2_gated): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (ac_conv2_gated_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (ac_conv2_sigmoid): Sigmoid()\n",
       "    (ac_conv3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (ac_conv3_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (ac_conv3_gated): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (ac_conv3_gated_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (ac_conv3_sigmoid): Sigmoid()\n",
       "    (ac_conv4): Conv2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (ac_conv4_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (ac_conv4_gated): Conv2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (ac_conv4_gated_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (ac_conv4_sigmoid): Sigmoid()\n",
       "    (ac_conv5): Conv2d(16, 4, kernel_size=(1, 4), stride=(1, 2), padding=(0, 1))\n",
       "    (ac_lin1): Linear(in_features=32, out_features=1, bias=True)\n",
       "    (ac_conv6): Conv2d(16, 1, kernel_size=(1, 4), stride=(1, 2), padding=(0, 1))\n",
       "    (ac_lin2): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       "  (L2_loss1): MSELoss()\n",
       "  (L2_loss2): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d or type(m) == nn.Linear or type(m) == nn.ConvTranspose2d:\n",
    "        torch.nn.init.xavier_normal_(m.weight.data)\n",
    "        \n",
    "CM_gen.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_params_ACVAE20.tar\")\n",
    "CM_gen.load_state_dict(checkpoint['model_state_dict'])\n",
    "CM_gen_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2onehot(labels, dim):\n",
    "       \"\"\"Convert label indices to one-hot vectors.\"\"\"\n",
    "       batch_size = labels.size(0)\n",
    "       out = torch.zeros(batch_size, dim)\n",
    "       out[np.arange(batch_size), labels.long()] = 1\n",
    "       return out.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-2adc38c7e1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCM_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_voice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_face\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mCM_gen_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs=25\n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    epoch += 1\n",
    "    \n",
    "    if (epoch == 7):\n",
    "        learning_rate = learning_rate_ \n",
    "        for param_group in CM_gen_optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "    if (epoch == 15):\n",
    "        learning_rate = learning_rate__\n",
    "        for param_group in CM_gen_optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "    if (epoch == 21):\n",
    "        learning_rate = learning_rate___\n",
    "        for param_group in CM_gen_optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    print('Epoch: %d' % epoch)\n",
    "    \n",
    "    avg_loss = 0\n",
    "    \n",
    "    data_len = len(data_iter)\n",
    "    \n",
    "    data_iter = iter(data_loader)\n",
    "\n",
    "    for (x_face, label_face) in data_iter:\n",
    "       \n",
    "        x_voice, label_voice = data_load_new(batchsize = 20,s = label_face)\n",
    "        \n",
    "        x_face = x_face.to(DEVICE)\n",
    "        label_face = label_face.to(DEVICE)\n",
    "        x_voice = x_voice.to(DEVICE)\n",
    "        label_voice = label_voice.to(DEVICE)\n",
    "        \n",
    "        label_real = label2onehot(labels=label_voice, dim=4)\n",
    "        \n",
    "        \n",
    "        CM_gen_optimizer.zero_grad()\n",
    "        \n",
    "        loss = CM_gen.calculate_loss(x_voice, x_face, label_real)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        CM_gen_optimizer.step()\n",
    "        \n",
    "        avg_loss = avg_loss + loss\n",
    "        \n",
    "        \n",
    "    CAVE_LOSS = avg_loss/data_len\n",
    "    print(\"avg_loss\", CAVE_LOSS)\n",
    "      \n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict':CM_gen.state_dict(),\n",
    "                    'optimizer_state_dict': CM_gen_optimizer.state_dict(),\n",
    "                    'loss': CAVE_LOSS,\n",
    "                    }, \"model_params_ACVAE\" + str(epoch) + \".tar\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = torch.tensor([[0,1]])\n",
    "print(s1.size())\n",
    "x, y = data_load_new(batchsize=1, s=s1)\n",
    "x = x.to(DEVICE)\n",
    "z = CM_gen.Voice_Classifier(x)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_real = label2onehot(labels=y, dim=4)\n",
    "print(label_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = torch.randn(1,8,1,1).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = xx[1,:,:,:].view(1,8,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.distributions.normal.Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "b=a.sample((1,8,1,1))\n",
    "xx=b.squeeze(4).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"xx_.npy\", xx.to('cpu').numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.load(\"xx.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8df87ec898>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s1 = torch.tensor([[1,1]])\n",
    "print(s1.size())\n",
    "x, y = data_load_new(batchsize=1, s=s1)\n",
    "\n",
    "ve_mu, ve_var = CM_gen.Voice_Encoder(x.to(DEVICE))\n",
    "\n",
    "\n",
    "reparam_VE = CM_gen.reparameterize(ve_mu, ve_var)\n",
    "\n",
    "recon_image_mu, recon_image_var = CM_gen.Face_Decoder(reparam_VE, xx)\n",
    "#print(recon_image_mu)\n",
    "recon_image = CM_gen.reparameterize(recon_image_mu, recon_image_var)\n",
    "#print(recon_image)\n",
    "\n",
    "recon1 = recon_image.view(3,64,64)\n",
    "\n",
    "recon1 = recon1.transpose(0,2)\n",
    "\n",
    "recon1 = recon1.transpose(0,1)\n",
    "\n",
    "y11 = recon1.to('cpu')\n",
    "plt.imshow(y11.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CM_gen.Face_Classifier(recon_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "s1 = torch.tensor([[0,1]])\n",
    "print(s1.size())\n",
    "x, y = data_load_new(batchsize=1, s=s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.to('cuda')\n",
    "\n",
    "CM_gen.Voice_Classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "##female young to male young\n",
    "\n",
    "voice_path_s = '/home/ubuntu/project2/voice_cvae/vcc2018/VCC2SF1/'\n",
    "file='10001.wav'\n",
    "voice_path_t = '/home/ubuntu/project2/voice_cvae/vcc2018/VCC2SM1/'\n",
    "\n",
    "output_voice_path= '/home/ubuntu/project2/voice_cvae/converted_voices/'\n",
    "\n",
    "source_dir = 'VCC2SF1'\n",
    "target_dir =  'VCC2SM1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_path_s = '/home/ubuntu/project2/CELEBA_DATA/MY_test/'\n",
    "#face_path_t = '/home/ubuntu/project2/CELEBA_DATA/FY_test/'\n",
    "face_file_s =  '004087.jpg'\n",
    "#face_file_s=   '006996.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "##male young to female old\n",
    "voice_path_s = '/home/ubuntu/project2/voice_cvae/vcc2018/VCC2SM1/'\n",
    "file='10015.wav'\n",
    "voice_path_t = '/home/ubuntu/project2/voice_cvae/vcc2018/VCC2SF4/'\n",
    "\n",
    "output_voice_path= '/home/ubuntu/project2/voice_cvae/converted_voices/'\n",
    "\n",
    "source_dir = 'VCC2SM1'\n",
    "target_dir =  'VCC2SF4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_path_s = '/home/ubuntu/project2/CELEBA_DATA/FO_test/'\n",
    "face_file_s=   '000381.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'test'\n",
    "batch_size = 4\n",
    "num_workers=1\n",
    "crop_size=178\n",
    "image_size=64\n",
    "\n",
    "transform = []\n",
    "if mode == 'train':\n",
    "    transform.append(T.RandomHorizontalFlip())\n",
    "transform.append(T.CenterCrop(crop_size))\n",
    "transform.append(T.Resize(image_size))\n",
    "transform.append(T.ToTensor())\n",
    "#transform.append(T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)))\n",
    "transform = T.Compose(transform)\n",
    "\n",
    "#image_target = Image.open(os.path.join(face_path_t, face_file_t))\n",
    "\n",
    "image_source = Image.open(os.path.join(face_path_s, face_file_s )) \n",
    "\n",
    "#face_target = transform(image_target)\n",
    "\n",
    "#face_target = face_target.unsqueeze(0)\n",
    "\n",
    "\n",
    "face_source = transform(image_source)\n",
    "\n",
    "face_source = face_source.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/project2/voice_cvae/preprocess.py:375: RuntimeWarning: divide by zero encountered in log\n",
      "  f0_converted = np.exp((np.log(f0) - mean_log_src) / std_log_src * std_log_target + mean_log_target)\n"
     ]
    }
   ],
   "source": [
    "wav, _ = librosa.load(os.path.join(voice_path_s, file), sr = sampling_rate, mono = True)\n",
    "wav = librosa.util.normalize(wav, norm=np.inf, axis=None)\n",
    "wav = wav_padding(wav = wav, sr = sampling_rate, frame_period = frame_period, multiple = 4)\n",
    "f0, timeaxis, sp, ap, mc = world_decompose(wav = wav, fs = sampling_rate, frame_period = frame_period)\n",
    "\n",
    "\n",
    "mc_transposed  = np.array(mc).T\n",
    "\n",
    "mcep_normalization_params_s = np.load(os.path.join(voice_path_s, \"mcep_\"+source_dir+\".npz\"))\n",
    "mcep_mean_s = mcep_normalization_params_s['mean']\n",
    "mcep_std_s = mcep_normalization_params_s['std']    \n",
    "mcep_normalization_params_t = np.load(os.path.join(voice_path_t, \"mcep_\"+target_dir+\".npz\"))\n",
    "mcep_mean_t = mcep_normalization_params_t['mean']\n",
    "mcep_std_t = mcep_normalization_params_t['std']\n",
    "\n",
    "mc_norm = (mc_transposed - mcep_mean_s) / mcep_std_s\n",
    "\n",
    "x = torch.Tensor(mc_norm).view(1, 1, mc_norm.shape[0], mc_norm.shape[1])\n",
    "\n",
    "n_frames=1024\n",
    "frames = np.shape(mc_transposed)[1]\n",
    "# #print(frames)\n",
    "start_ = np.random.randint(frames - n_frames + 1)\n",
    "end_ = start_ + n_frames\n",
    "x=x[:,:,:,start_:end_]\n",
    "\n",
    "\n",
    "x = x.to(DEVICE)\n",
    "#face_target= face_target.to(DEVICE)\n",
    "\n",
    "face_source = face_source.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "#(face,_) in data_iter\n",
    "\n",
    "CM_gen.eval()\n",
    "\n",
    "UE_mu, UE_var = CM_gen.Utterance_Encoder(x)\n",
    "FE_mu, FE_var = CM_gen.Face_Encoder(face_target)\n",
    "reparam_UE = CM_gen.reparameterize(UE_mu, UE_var)\n",
    "reparam_FE = CM_gen.reparameterize(FE_mu, FE_var)\n",
    "UD_mu, UD_var = CM_gen.Utterance_Decoder(reparam_UE,reparam_FE)\n",
    "recon_voice = CM_gen.reparameterize(UD_mu, UD_var)\n",
    "\n",
    "\n",
    "FE_mu_s, FE_var_s = CM_gen.Face_Encoder(face_source)\n",
    "reparam_FE_s = CM_gen.reparameterize(FE_mu_s, FE_var_s)\n",
    "UD_mu_s, UD_var_s = CM_gen.Utterance_Decoder(reparam_UE,reparam_FE_s)\n",
    "recon_voice_s = CM_gen.reparameterize(UD_mu_s, UD_var_s)\n",
    "\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    z_dec_s = recon_voice_s.data.cpu().numpy().reshape((mc_norm.shape[0], n_frames))\n",
    "else:\n",
    "    z_dec_s = recon_voice_s.data.numpy().reshape((mc_norm.shape[0], n_frames))\n",
    "\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    z_dec_t = recon_voice.data.cpu().numpy().reshape((mc_norm.shape[0], n_frames))\n",
    "else:\n",
    "    z_dec_t = recon_voice.data.numpy().reshape((mc_norm.shape[0], n_frames))\n",
    "    \n",
    "\n",
    "    \n",
    "mc_converted_t = z_dec_t * mcep_std_t + mcep_mean_t\n",
    "mc_converted_t = mc_converted_t.T\n",
    "mc_converted_t = np.ascontiguousarray(mc_converted_t)\n",
    "sp_converted_t = world_decode_mc(mc = mc_converted_t, fs = sampling_rate)\n",
    "\n",
    "\n",
    "mc_converted_s = z_dec_s * mcep_std_s + mcep_mean_s\n",
    "mc_converted_s = mc_converted_s.T\n",
    "mc_converted_s = np.ascontiguousarray(mc_converted_s)\n",
    "sp_converted_s = world_decode_mc(mc = mc_converted_s, fs = sampling_rate)\n",
    "\n",
    "sp = sp[:n_frames,:]\n",
    "\n",
    "sp_gained = np.multiply(sp, np.divide(sp_converted_t, sp_converted_s))\n",
    "\n",
    "logf0s_normalization_params_s = np.load(os.path.join(voice_path_s, \"log_f0_\"+source_dir+\".npz\"))\n",
    "logf0s_mean_s = logf0s_normalization_params_s['mean']\n",
    "logf0s_std_s = logf0s_normalization_params_s['std']\n",
    "\n",
    "\n",
    "logf0s_normalization_params_t = np.load(os.path.join(voice_path_t, \"log_f0_\"+target_dir+\".npz\"))\n",
    "logf0s_mean_t = logf0s_normalization_params_t['mean']\n",
    "logf0s_std_t = logf0s_normalization_params_t['std']\n",
    "\n",
    "\n",
    "\n",
    "ap=ap[:n_frames]\n",
    "\n",
    "\n",
    "\n",
    "f0_converted = pitch_conversion(f0 = f0, mean_log_src = logf0s_mean_s, std_log_src = logf0s_std_s, mean_log_target = logf0s_mean_t, std_log_target = logf0s_std_t)\n",
    "\n",
    "f0_converted = f0_converted[:n_frames]\n",
    "wav_transformed = world_speech_synthesis(f0 = f0_converted, sp = sp_gained, ap = ap, fs = sampling_rate, frame_period = frame_period)\n",
    "librosa.output.write_wav(os.path.join(output_voice_path, source_dir +\"_to_\"+target_dir+\".wav\"), wav_transformed, sampling_rate)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
